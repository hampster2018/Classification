{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"https://git.io/nlp-with-transformers\"\n",
    "df_issues = pd.read_json(dataset, lines=True) \n",
    "print(f\"DataFrame shape: {df_issues.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"url\", \"id\", \"title\", \"user\", \"labels\", \"state\", \"created_at\", \"body\"]\n",
    "df_issues.loc[2, cols].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"labels\"] = (df_issues[\"labels\"]\n",
    "                       .apply(lambda x: [meta[\"name\"] for meta in x]))\n",
    "df_issues[[\"labels\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"labels\"].apply(lambda x : len(x)).value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_issues[\"labels\"].explode().value_counts()\n",
    "print(f\"Number of labels: {len(df_counts)}\")\n",
    "# Display the top-8 label categories\n",
    "df_counts.to_frame().head(8).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"Core: Tokenization\": \"tokenization\",\n",
    "             \"New model\": \"new model\",\n",
    "             \"Core: Modeling\": \"model training\",\n",
    "             \"Usage\": \"usage\",\n",
    "             \"Core: Pipeline\": \"pipeline\",\n",
    "             \"TensorFlow\": \"tensorflow or tf\",\n",
    "             \"PyTorch\": \"pytorch\",\n",
    "             \"Examples\": \"examples\",\n",
    "             \"Documentation\": \"documentation\"}\n",
    "\n",
    "def filter_labels(x):\n",
    "    return [label_map[label] for label in x if label in label_map]\n",
    "\n",
    "df_issues[\"labels\"] = df_issues[\"labels\"].apply(filter_labels)\n",
    "all_labels = list(label_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_issues[\"labels\"].explode().value_counts()\n",
    "df_counts.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"split\"] = \"unlabeled\"\n",
    "mask = df_issues[\"labels\"].apply(lambda x: len(x)) > 0\n",
    "df_issues.loc[mask, \"split\"] = \"labeled\"\n",
    "df_issues[\"split\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\"title\", \"body\", \"labels\"]:\n",
    "    print(f\"{column}: {df_issues[column].iloc[26][:500]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"text\"] = (df_issues\n",
    "                     .apply(lambda x: x[\"title\"] + \"\\n\\n\" + x[\"body\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before = len(df_issues)\n",
    "df_issues = df_issues.drop_duplicates(subset=\"text\")\n",
    "print(f\"Removed {(len_before-len(df_issues))/len_before:.2%} duplicates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_labels])\n",
    "mlb.transform([[\"tokenization\", \"new model\"], [\"pytorch\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "skmultilearn = importlib.import_module(\"skmultilearn\")\n",
    "import numpy as np\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "def balanced_split(df, test_size=0.5):\n",
    "    ind = np.expand_dims(np.arange(len(df)), axis=1)\n",
    "    labels = mlb.transform(df[\"labels\"])\n",
    "    ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels,\n",
    "                                                           test_size)\n",
    "    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_clean = df_issues[[\"text\", \"labels\", \"split\"]].reset_index(drop=True).copy()\n",
    "df_unsup = df_clean.loc[df_clean[\"split\"] == \"unlabeled\", [\"text\", \"labels\"]]\n",
    "df_sup = df_clean.loc[df_clean[\"split\"] == \"labeled\", [\"text\", \"labels\"]]\n",
    "\n",
    "np.random.seed(0)\n",
    "df_train, df_tmp = balanced_split(df_sup, test_size=0.5)\n",
    "df_valid, df_test = balanced_split(df_tmp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\n",
    "    \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)),\n",
    "    \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "all_indices = np.expand_dims(list(range(len(ds[\"train\"]))), axis=1)\n",
    "indices_pool = all_indices\n",
    "labels = mlb.transform(ds[\"train\"][\"labels\"])\n",
    "train_samples = [8, 16, 32, 64, 128]\n",
    "train_slices, last_k = [], 0\n",
    "\n",
    "for i, k in enumerate(train_samples):\n",
    "    # Split off samples necessary to fill the gap to the next split size\n",
    "    indices_pool, labels, new_slice, _ = iterative_train_test_split(\n",
    "        indices_pool, labels, (k-last_k)/len(labels))\n",
    "    last_k = k\n",
    "    if i==0: train_slices.append(new_slice)\n",
    "    else: train_slices.append(np.concatenate((train_slices[-1], new_slice)))\n",
    "\n",
    "# Add full dataset as last slice\n",
    "train_slices.append(all_indices), train_samples.append(len(ds[\"train\"]))\n",
    "train_slices = [np.squeeze(train_slice) for train_slice in train_slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target split sizes:\")\n",
    "print(train_samples)\n",
    "print(\"Actual split sizes:\")\n",
    "print([len(x) for x in train_slices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of Naive Bayes Classifier\n",
    "def prepare_labels(batch):\n",
    "    batch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(prepare_labels, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# micro tracks scores across frequent labels, macro tracks on all label regardless of frequency\n",
    "macro_scores, micro_scores = defaultdict(list), defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "for train_slice in train_slices:\n",
    "    # Takes in the training slice and gets the data on that slice\n",
    "    ds_train_sample = ds[\"train\"].select(train_slice)\n",
    "    y_train = np.array(ds_train_sample[\"label_ids\"])\n",
    "    y_test = np.array(ds[\"test\"][\"label_ids\"])\n",
    "\n",
    "    # Counts the text as a token inside of a vectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(ds_train_sample[\"text\"])\n",
    "    X_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\n",
    "\n",
    "    # Creates and trains the model\n",
    "    classifier = BinaryRelevance(classifier=MultinomialNB())\n",
    "    classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "    # Predicts how it will do and evaluates the results\n",
    "    y_pred_test = classifier.predict(X_test_counts)\n",
    "    clf_report = classification_report(\n",
    "        y_test, y_pred_test, target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
    "    \n",
    "    # Stores the training slice as a metric\n",
    "    macro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
    "    micro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start of the Vanilla Transformer\n",
    "\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoConfig, AutoModelForSequenceClassification)\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\n",
    "ds_enc = ds.map(tokenize, batched=True)\n",
    "ds_enc = ds_enc.remove_columns(['labels', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Going to change the type of a column in order to be float. Can't change type directly because it comes declared\n",
    "\n",
    "ds_enc.set_format(\"torch\")\n",
    "ds_enc = ds_enc.map(lambda x: {\"label_ids_f\": x[\"label_ids\"].to(torch.float)}, \n",
    "                    remove_columns=[\"label_ids\"])\n",
    "\n",
    "ds_enc = ds_enc.rename_column(\"label_ids_f\", \"label_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the settings on the transformer to fit the smaller data set\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args_fine_tune = TrainingArguments(\n",
    "    output_dir=\"./results\", num_train_epochs=20, learning_rate=3e-5,\n",
    "    lr_scheduler_type='constant', per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=32, weight_decay=0.0,\n",
    "    evaluation_strategy=\"epoch\", save_strategy=\"epoch\",logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, metric_for_best_model='micro f1',\n",
    "    save_total_limit=1, log_level='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = sigmoid(pred.predictions)\n",
    "    y_pred = (y_pred > .5).astype(float)\n",
    "\n",
    "    clf_dict = classification_report(y_true, y_pred, target_names=all_labels,\n",
    "                                    zero_division=0, output_dict=True)\n",
    "    \n",
    "    return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\n",
    "            \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "config.num_labels = len(all_labels)\n",
    "config.problem_type = \"multi_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_slice in train_slices:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, tokenizer=tokenizer,\n",
    "        args=training_args_fine_tune,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=ds_enc[\"train\"].select(train_slice),\n",
    "        eval_dataset=ds_enc[\"valid\"],)\n",
    "    \n",
    "    trainer.train()\n",
    "    pred = trainer.predict(ds_enc[\"test\"])\n",
    "    metrics = compute_metrics(pred)\n",
    "    macro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"macro f1\"])\n",
    "    micro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"micro f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "    for run in micro_scores.keys():\n",
    "        if run == current_model:\n",
    "            ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)\n",
    "            ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)\n",
    "        else:\n",
    "            ax0.plot(sample_sizes, micro_scores[run], label=run,\n",
    "                     linestyle=\"dashed\")\n",
    "            ax1.plot(sample_sizes, macro_scores[run], label=run,\n",
    "                     linestyle=\"dashed\")\n",
    "\n",
    "    ax0.set_title(\"Micro F1 scores\")\n",
    "    ax1.set_title(\"Macro F1 scores\")\n",
    "    ax0.set_ylabel(\"Test set F1 score\")\n",
    "    ax0.legend(loc=\"lower right\")\n",
    "    for ax in [ax0, ax1]:\n",
    "        ax.set_xlabel(\"Number of training samples\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xticks(sample_sizes)\n",
    "        ax.set_xticklabels(sample_sizes)\n",
    "        ax.minorticks_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")\n",
    "plot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (vanilla)\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
